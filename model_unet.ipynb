{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "former-garlic",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "class down_conv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(down_conv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(ch_in),\n",
    "            nn.Conv2d(in_channels=ch_in, out_channels=ch_out, kernel_size=3, stride=1, padding=(1,1), bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.Conv2d(in_channels=ch_out, out_channels=ch_out, kernel_size=3, stride=1, padding=(1,1), bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class out_conv(nn.Module):\n",
    "    def __init__(self, ch_in):\n",
    "        super(out_conv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(ch_in),\n",
    "            nn.Conv2d(in_channels=ch_in, out_channels=1, kernel_size=3, stride=1, padding=(1,1), bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class up_T_conv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(up_T_conv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=ch_in),\n",
    "            nn.ConvTranspose2d(in_channels=ch_in, out_channels=ch_out, kernel_size=(2,2), stride=(2,2), padding=0) # (1,1))\n",
    "            # TODO: ConvTranspose2d的padding怎么设置??\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self, ch_in):\n",
    "        super(up_conv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=ch_in),\n",
    "            nn.Conv2d(in_channels=ch_in, out_channels=ch_in, kernel_size=3, stride=1, padding=(1,1), bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.channel_layers = [64, 128, 256, 512]\n",
    "        \n",
    "        self.MaxPool = nn.MaxPool2d(kernel_size=(2,2), stride=2, ceil_mode=True)\n",
    "        \n",
    "        self.DownConv0 = down_conv(6, 64)\n",
    "        self.DownConv1 = down_conv(64, 128)\n",
    "        self.DownConv2 = down_conv(128, 256)\n",
    "        self.DownConv3 = down_conv(256, 512)\n",
    "        \n",
    "        self.GetOut = nn.ModuleList([out_conv(self.channel_layers[i]*2) for i in range(0,3)] + [out_conv(self.channel_layers[3])])\n",
    "        # 64*2->1 128*2->1 256*2->1 512->1\n",
    "        \n",
    "        self.Up_T_Conv = nn.ModuleList([up_T_conv(self.channel_layers[i]*2, self.channel_layers[i-1]) for i in range(1,3)]+[up_T_conv(self.channel_layers[3], self.channel_layers[3-1])])\n",
    "        # 'placeholder' 128*2->64 256*2->128 512->256\n",
    "        \n",
    "        self.Up_Conv = nn.ModuleList([up_conv(self.channel_layers[i]*2) for i in range(3)])\n",
    "        # 64*2 128*2 256*2\n",
    "        \n",
    "        self.SoftmaxLayer = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x, out_floor):\n",
    "        save_for_concat = []\n",
    "        x = self.DownConv0(x)\n",
    "        save_for_concat.append(x)\n",
    "        x = self.MaxPool(x)\n",
    "        x = self.DownConv1(x)\n",
    "        save_for_concat.append(x)\n",
    "        x = self.MaxPool(x)\n",
    "        x = self.DownConv2(x)\n",
    "        save_for_concat.append(x)\n",
    "        x = self.MaxPool(x)\n",
    "        x = self.DownConv3(x)\n",
    "        \n",
    "        for floor in range(3,-1,-1): # floor: 3 2 1 0\n",
    "            #print(f'[floor = {floor}]')\n",
    "            if out_floor == floor:\n",
    "                x = self.GetOut[floor](x)\n",
    "                return x\n",
    "            else:\n",
    "                x = self.Up_T_Conv[floor-1](x)\n",
    "                if x.shape[-1] != save_for_concat[floor-1].shape[-1]:\n",
    "                    if x.shape[-1]-1 == save_for_concat[floor-1].shape[-1]:\n",
    "                        x = x[:,:,:,:-1]\n",
    "                    assert(x.shape[-1] == save_for_concat[floor-1].shape[-1])\n",
    "                    #print(f'after cut, x.shape = {x.shape}')\n",
    "                #else:\n",
    "                    #print(f'no need to cut, x.shape = {x.shape}')\n",
    "                x = self.Up_Conv[floor-1](torch.cat((save_for_concat[floor-1], x), dim=1)) # 可能有维数问题\n",
    "                #print(x.shape)\n",
    "\n",
    "        x = self.SoftmaxLayer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "mexican-messaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n",
      "[NbConvertApp] Converting notebook model_unet.ipynb to python\n",
      "[NbConvertApp] Writing 4828 bytes to model_unet.py\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    !jupyter nbconvert --to python model_unet.ipynb\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-toyota",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
