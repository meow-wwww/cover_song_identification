{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "contrary-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "原始函数，CrossEntropyLoss_Origin的内核\n",
    "def cross_entropy_multilayer_simplify(sm, one_hot): \n",
    "    # sm one_hot都是多片的\n",
    "    # 论文原始版本\n",
    "        # 可以优化的方向：对unvoicing的部分加以控制：平方误差or dual loss?\n",
    "    assert(sm.shape == one_hot.shape)\n",
    "    batch_size, case, _ = sm.shape\n",
    "    print(batch_size, case)\n",
    "    none_zero_lines = one_hot.bool().any(len(one_hot.shape)-1)\n",
    "    # print(none_zero_lines)\n",
    "    return (-torch.log(sm)*one_hot).sum()/none_zero_lines.sum()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "split-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss_Origin(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss_Origin, self).__init__()\n",
    "        return\n",
    "    def forward(self, output, one_hot):\n",
    "        '''\n",
    "        output: network output(not softmax yet)\n",
    "                shape: [N, 1, f, t]\n",
    "        one_hot: ground truth(has not been gaussian blurred)\n",
    "                shape: [N, f, t]\n",
    "        [OUTPUT]是一个batch中不同样本的均值\n",
    "        '''\n",
    "        output = output.squeeze(dim=1) # output: [N, f, t]\n",
    "        \n",
    "        output_minus = output - output.max() # 防止softmax溢出\n",
    "        sm = func.softmax(output_minus, dim=-2)\n",
    "\n",
    "        none_zero_lines_num = one_hot.bool().any(1).sum()\n",
    "        \n",
    "        if none_zero_lines_num == 0: # 在这里曾经因为除以0发生了一次NaN (╥﹏╥)\n",
    "            return (-torch.log(sm)*one_hot).sum()*0 # 其实直接return一个0tensor也行，但考虑到device，还是这样写吧\n",
    "        else:\n",
    "            return (-torch.log(sm)*one_hot).sum()/none_zero_lines_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "involved-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss_for_FA_CE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss_for_FA_CE, self).__init__()\n",
    "        return\n",
    "    def forward(self, output, one_hot):\n",
    "        '''\n",
    "        output: network output(not softmax yet)\n",
    "                shape: [N, 1, f, t]\n",
    "        one_hot: ground truth(has not been gaussian blurred)\n",
    "                shape: [N, f, t]\n",
    "        [OUTPUT]是一个batch中不同样本的均值\n",
    "        '''\n",
    "        output = output.squeeze(dim=1) # output: [N, f, t]\n",
    "        output_minus = output - output.max() # 防止softmax溢出\n",
    "        sm = func.softmax(output_minus, dim=-2)\n",
    "        \n",
    "        # 对one-hot进行处理，没有label的全零列改成1/360\n",
    "\n",
    "        none_zero_lines = one_hot.bool().any(1).reshape(-1)\n",
    "        one_hot = one_hot.float()\n",
    "        index0 = torch.tensor([list(range(one_hot.shape[0]))]*one_hot.shape[2]).T.reshape(-1)[none_zero_lines == False]\n",
    "        index2 = torch.tensor([list(range(one_hot.shape[2]))]*one_hot.shape[0]).reshape(-1)[none_zero_lines == False]\n",
    "        one_hot[index0, :, index2] = 1/one_hot.shape[1]\n",
    "        \n",
    "        return (-torch.log(sm)*one_hot).sum()/(output.shape[2]*output.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "powered-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n",
      "[NbConvertApp] Converting notebook loss_function.ipynb to python\n",
      "[NbConvertApp] Writing 1611 bytes to loss_function.py\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    !jupyter nbconvert --to python loss_function.ipynb\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-light",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
