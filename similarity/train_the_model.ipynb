{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "111252fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import random\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse, os, math\n",
    "import numpy as np\n",
    "from hpcp_loader_for_softdtw import *\n",
    "import models.BaseSPPNet as models\n",
    "from config import DefaultConfig, opt\n",
    "from utility import *\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import resource\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00cc1d",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# argparse\n",
    "save_dir = None\n",
    "saved_model_path = None\n",
    "lr = 1e-3\n",
    "weight_decay = 0\n",
    "num_workers = 0\n",
    "epochs = 0\n",
    "epochs_finished = 0\n",
    "BATCH_SIZE = 16\n",
    "device_ids = [0]\n",
    "datatype = None\n",
    "train_scale = None\n",
    "test_scale = None\n",
    "test_source = None\n",
    "train_cut = None\n",
    "\n",
    "print('--------------ArgParse--------------')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-d', '--save_dir', help='目标存储目录')\n",
    "parser.add_argument('-s', '--saved_model', help='以训练过的模型')\n",
    "parser.add_argument('--lr', type=float, help='学习率')\n",
    "parser.add_argument('--weight_decay', type=float, help='正则化系数')\n",
    "parser.add_argument('--workers', type=int, help='num_workers')\n",
    "parser.add_argument('-e', '--epochs', type=int, help='有几个epoch')\n",
    "parser.add_argument('-g', '--gpu', help='要用的gpu号')\n",
    "parser.add_argument('-b', '--batch_size', type=int, help='batch_size')\n",
    "parser.add_argument('--datatype', help='训练数据的形式,f0,multif0,cqt等')\n",
    "parser.add_argument('--train_scale', help='训练集大小尺度')\n",
    "parser.add_argument('--test_scale', help='测试集大小尺度')\n",
    "parser.add_argument('--test_source', help='测试用数据来源')\n",
    "parser.add_argument('--train_cut', help='训练数据切割方式')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "assert args.save_dir != None, ('请输入存储目录')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    print(f'| Save_dir {args.save_dir} does not exist. Will create one.')\n",
    "    save_dir = args.save_dir\n",
    "    os.mkdir(save_dir)\n",
    "else:\n",
    "    print(f'| Save_dir {args.save_dir} already exist. Will save to this directory.')\n",
    "    save_dir = args.save_dir\n",
    "\n",
    "if args.saved_model == None:\n",
    "    print('| train from scratch')\n",
    "else:\n",
    "    print(f'| saved model:\\t{args.saved_model}')\n",
    "saved_model_path = args.saved_model\n",
    "\n",
    "if args.lr != None:\n",
    "    lr = args.lr\n",
    "print(f'| lr:\\t{lr}')\n",
    "    \n",
    "if args.weight_decay != None:\n",
    "    weight_decay = args.weight_decay\n",
    "print(f'| weight_dacay:\\t{weight_decay}')\n",
    "    \n",
    "assert args.workers != None, ('请输入workers数')\n",
    "num_workers = args.workers\n",
    "print(f'| num_workers:\\t{num_workers}')\n",
    "\n",
    "assert args.epochs != None, ('请输入epochs数')\n",
    "epochs = args.epochs\n",
    "print(f'| epochs:\\t{epochs}')\n",
    "\n",
    "assert args.gpu != None, ('请输入要用的gpu号')\n",
    "device_ids = list(range(len(args.gpu.split(','))))\n",
    "print(f'| gpu:\\t{args.gpu}')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "print(f'| device count:\\t{torch.cuda.device_count()}')\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "if args.batch_size != None:\n",
    "    BATCH_SIZE = args.batch_size\n",
    "print(f'| batch_size:\\t{BATCH_SIZE}')\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "print(f'| loss_function:\\t{loss_fn.__class__.__name__}')\n",
    "\n",
    "assert args.datatype != None\n",
    "datatype = args.datatype\n",
    "print(f'| datatype:\\t{datatype}')\n",
    "\n",
    "train_scale = args.train_scale\n",
    "print(f'| train_scale:\\t{train_scale}')\n",
    "\n",
    "test_scale = args.test_scale\n",
    "print(f'| test_scale:\\t{test_scale}')\n",
    "\n",
    "assert args.test_source != None\n",
    "test_source = args.test_source\n",
    "print(f'| test_source:\\t{test_source}')\n",
    "\n",
    "assert args.train_cut in ['random', 'front'] or args.train_cut[0:11] == 'semi-random'\n",
    "train_cut = args.train_cut\n",
    "print(f'| train_cut:\\t{train_cut}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e7b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.NeuralDTW_CNN_Mask_dilation_SPP6(None)\n",
    "model = model.cuda()\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "if saved_model_path != None:\n",
    "    print(f'loading model from {saved_model_path}...')\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    \n",
    "from torch.nn import init\n",
    "#define the initial function to init the layer's parameters for the network\n",
    "def initNetParams(net):\n",
    "    '''Init net parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.xavier_uniform(m.weight)\n",
    "            if m.bias != None:\n",
    "                init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant(m.weight, 1)\n",
    "            init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal(m.weight, std=1e-3)\n",
    "            if m.bias != None:\n",
    "                init.constant(m.bias, 0)\n",
    "initNetParams(model)\n",
    "\n",
    "loss_fn = nn.BCELoss() # loss_fn返回的值已经对每个样本做平均了\n",
    "loss_fn = loss_fn.cuda()\n",
    "loss_fn = nn.DataParallel(loss_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=10,\n",
    "                                                               verbose=True, min_lr=5e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de457c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-10 15:53:35.606070 - Preparing train_dataloader...\n",
      "2021-10-10 15:53:35.610179 - Preparing test_dataloader...\n"
     ]
    }
   ],
   "source": [
    "# Dataloader\n",
    "print(f'{datetime.datetime.now()} - Preparing train_dataloader...')\n",
    "train_data = triplet_CQT(out_length=400, is_label=True, mode=(datatype, train_scale), cut=train_cut, is_random=True)\n",
    "train_dataloader = DataLoader(train_data, BATCH_SIZE, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "print(f'{datetime.datetime.now()} - Preparing test_dataloader...')\n",
    "test_data = CQT(mode=(test_source, datatype, test_scale), out_length=400)\n",
    "test_dataloader = DataLoader(test_data, BATCH_SIZE, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28f7359b",
   "metadata": {
    "code_folding": [
     34
    ]
   },
   "outputs": [],
   "source": [
    "# 一个epoch的训练/测试\n",
    "def train(dataloader, model, loss_fn, optimizer, scheduler):\n",
    "    model.train()\n",
    "    model.module.model.train()\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    batch_num = math.ceil(size/dataloader.batch_size)\n",
    "    \n",
    "    loss_total = 0\n",
    "    num = 0\n",
    "    \n",
    "    for batch, (a, p, n, la, lp, ln) in tqdm(enumerate(dataloader)):\n",
    "        B, _, _, _ = a.shape\n",
    "        target = torch.cat((torch.ones(B), torch.zeros(B))).cuda()\n",
    "        a, p, n = a.cuda(device=device_ids[0]), p.cuda(device=device_ids[0]), n.cuda(device=device_ids[0])\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(a, p, n)\n",
    "        pred = pred.squeeze(1)\n",
    "#         print(pred, target)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss = loss.sum() # 注意,这里的loss不是平均每帧的loss,而是(平均每帧的loss*gpu数)\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "#         print(list(model.named_parameters())[0][1].grad[0])\n",
    "        optimizer.step()\n",
    "\n",
    "        loss /= len(device_ids)\n",
    "        loss_total += loss.item()\n",
    "        num += B\n",
    "\n",
    "        if (batch+1) % 100 == 0:\n",
    "            print(f\"Avg loss: {loss.item():.4f}  [{num:>5d}/{size:>5d}]\")\n",
    "            scheduler.step(loss.item())\n",
    "\n",
    "    return loss_total/batch_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5cb9a0b7",
   "metadata": {
    "code_folding": [
     43
    ]
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_slow_batch(softdtw, dataloader, batch=50, is_dis=False):\n",
    "    softdtw.eval()\n",
    "    softdtw.module.model.eval()\n",
    "    seqs, labels = [], []\n",
    "    for ii, (data, label) in tqdm(enumerate(dataloader)):\n",
    "        input = data.cuda(device=device_ids[0])\n",
    "        seqs.append(input)\n",
    "        labels.append(label)\n",
    "    seqs = torch.cat(seqs, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    N = labels.shape[0]\n",
    "    if N == 350:\n",
    "        query_l = [i // 100 for i in range(100 * 100, 350 * 100)]\n",
    "        ref_l = [i for i in range(100)] * 250\n",
    "    else:\n",
    "        query_l = [i // N for i in range(N * N)] # [0,...,0,1,...,1,...,N,...,N]\n",
    "        ref_l = [i for i in range(N)] * N # [0,...,N,0,...,N,...,0,...,N]\n",
    "    dis2d = np.zeros((N, N))\n",
    "\n",
    "    N = N * N if N != 350 else 100 * 250 # N = N*N\n",
    "    for st in tqdm(range(0, N, batch)):\n",
    "        fi = (st + batch) if st + batch <= N else N\n",
    "        query = seqs[query_l[st: fi], :, :]\n",
    "        ref = seqs[ref_l[st: fi], :, :]\n",
    "        s = softdtw.module.multi_compute_s(query, ref).data.cpu().numpy()\n",
    "        for k in range(st, fi):\n",
    "            i, j = query_l[k], ref_l[k]\n",
    "            if is_dis:\n",
    "                dis2d[i, j] = s[k - st]\n",
    "            else:\n",
    "                dis2d[i, j] = -s[k - st]\n",
    "    if len(labels) == 350:\n",
    "        MAP, top10, rank1 = calc_MAP(dis2d, labels, [100, 350])\n",
    "    else:\n",
    "        MAP, top10, rank1 = calc_MAP(dis2d, labels)\n",
    "    print(f'MAP:\\t{MAP:.4f}\\ttop10:\\t{top10:.4f}\\trank1:\\t{rank1:.4f}')\n",
    "\n",
    "    softdtw.train()\n",
    "    softdtw.module.model.train()\n",
    "    return MAP\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_quick(softdtw, dataloader):\n",
    "    softdtw.eval()\n",
    "    softdtw.module.model.eval()\n",
    "    labels = []\n",
    "    temp = []\n",
    "    count = -1\n",
    "    for ii, (data, label) in tqdm(enumerate(dataloader)):\n",
    "        labels.append(label)\n",
    "    labels = torch.cat(labels, dim=0) # labels: 库里每首歌的song_id\n",
    "    N = labels.shape[0]\n",
    "    dis2d = np.zeros((N, N))\n",
    "    for ii, (data, label) in tqdm(enumerate(dataloader)):\n",
    "        data = data.cuda(device=device_ids[0])\n",
    "        count += 1\n",
    "        if count == 0: # for中的第一轮循环\n",
    "            temp.append((data, count)) # temp[0] = (data, 0)\n",
    "        else:\n",
    "            for i in range(len(temp)):\n",
    "                dis = softdtw.module.multi_compute_s(data, temp[i][0]).data.cpu().numpy()\n",
    "                dis2d[temp[i][1]][count], dis2d[count][temp[i][1]] = -dis, -dis\n",
    "            temp.append((data, count))\n",
    "\n",
    "    MAP, top10, rank1 = calc_MAP(dis2d[0:labels.shape[0], 0:labels.shape[0]], labels)\n",
    "    print(f'MAP:\\t{MAP:.4f}\\ttop10:\\t{top10:.4f}\\trank1:\\t{rank1:.4f}')\n",
    "    softdtw.train()\n",
    "    softdtw.module.model.train()\n",
    "    return MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e35f7",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "test_MAP_list = []\n",
    "best_MAP = val_slow_batch(model, test_dataloader)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------{datetime.datetime.now()}\")\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer, scheduler)\n",
    "    print(f'train_loss:{train_loss:.6f}')\n",
    "\n",
    "    MAP = 0\n",
    "    print('gdoras_test:')\n",
    "    MAP += val_slow_batch(model, test_dataloader)\n",
    "    # test on various dataset\n",
    "    # print(\"Youtube350:\")\n",
    "    # MAP += val_slow_batch(model, val_dataloader, batch=100, is_dis=kwargs['zo'])\n",
    "    # print(\"CoverSong80:\")\n",
    "    # MAP += val_slow_batch(model, val_dataloader80, batch=100, is_dis=kwargs['zo'])\n",
    "    # print(\"Youtube350:\")\n",
    "    # MAP += val_quick(model, val_dataloader_350)\n",
    "    # print(\"CoverSong80:\")\n",
    "    # MAP += val_quick(model, val_dataloader_80)\n",
    "    # print(\"Mazurkas:\")\n",
    "    # MAP += val_slow_batch(model, val_dataloader_marukars, batch=100, is_dis=kwargs['zo'])\n",
    "    if MAP > best_MAP:\n",
    "        print('*****************BEST*****************')\n",
    "        print(f'[epoch {t+1}] {best_MAP:.4f} --- {MAP:.4f}. Save.')\n",
    "        best_MAP = MAP\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f'model_best.pth'))\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'latest.pth'))\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_MAP_list.append(MAP)\n",
    "    # valid_oa_list.append(oa)\n",
    "    # test_oa_list.append(test_oa)\n",
    "    \n",
    "    if optimizer.state_dict()['param_groups'][0]['lr']<=0:\n",
    "        print(f'Early stop after {t+1} epochs.')\n",
    "        break\n",
    "    \n",
    "    plt.cla()\n",
    "    plt.figure(figsize=(7,10))\n",
    "    ax = plt.subplot(2,1,1)\n",
    "    ax.set_title('train_loss')\n",
    "    plt.plot(range(1,len(train_loss_list)+1), train_loss_list, 'b')\n",
    "    ax = plt.subplot(2,1,2)\n",
    "    ax.set_title('test_MAP')\n",
    "    plt.plot(range(1,len(test_MAP_list)+1), test_MAP_list, 'r')\n",
    "    plt.savefig(os.path.join(save_dir, 'loss_and_MAP.png'))\n",
    "    \n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
