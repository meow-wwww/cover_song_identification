Using cuda device
--------------ArgParse--------------
Save_dir ./0526_f3 already exist. Will create ./0526_f3_2 to avoid overwrite.
Don't use saved model, train from scratch
Using lr from command: lr=0.0001
2021-05-26 20:10:57.078843 - Preparing train_dataloader...
2021-05-26 20:11:05.504539 - Preparing valid_dataloader...
Adjusting learning rate of group 0 to 1.0000e-04.
Epoch 1
-------------------------------2021-05-26 20:11:07.748196
Avg loss: 101.050285  [   16/ 7627]
Avg loss: 37.669731  [  336/ 7627]
Avg loss: 31.122623  [  656/ 7627]
Avg loss: 23.349720  [  976/ 7627]
Avg loss: 23.850931  [ 1296/ 7627]
Avg loss: 16.946609  [ 1616/ 7627]
Avg loss: 24.529995  [ 1936/ 7627]
Avg loss: 25.931343  [ 2256/ 7627]
Avg loss: 14.886034  [ 2576/ 7627]
Avg loss: 18.958214  [ 2896/ 7627]
Avg loss: 36.363853  [ 3216/ 7627]
Avg loss: 23.372215  [ 3536/ 7627]
Avg loss: 21.434713  [ 3856/ 7627]
Avg loss: 16.456818  [ 4176/ 7627]
Avg loss: 11.292923  [ 4496/ 7627]
Avg loss: 30.978876  [ 4816/ 7627]
Avg loss: 20.921005  [ 5136/ 7627]
Avg loss: 23.481308  [ 5456/ 7627]
Avg loss: 27.956097  [ 5776/ 7627]
Avg loss: 22.016302  [ 6096/ 7627]
Avg loss: 14.227516  [ 6416/ 7627]
Avg loss: 30.782181  [ 6736/ 7627]
Avg loss: 18.202770  [ 7056/ 7627]
Avg loss: 20.542280  [ 7376/ 7627]
Adjusting learning rate of group 0 to 9.4000e-05.
Test Error: Avg loss: 27.069265 

Epoch 2
-------------------------------2021-05-26 20:14:15.388993
Avg loss: 22.199051  [   16/ 7627]
Avg loss: 21.248697  [  336/ 7627]
Avg loss: 19.186668  [  656/ 7627]
Avg loss: 14.890186  [  976/ 7627]
Avg loss: 18.611633  [ 1296/ 7627]
Avg loss: 17.421249  [ 1616/ 7627]
Avg loss: 14.636127  [ 1936/ 7627]
Avg loss: 27.006918  [ 2256/ 7627]
Avg loss: 15.433367  [ 2576/ 7627]
Avg loss: 15.710831  [ 2896/ 7627]
Avg loss: 12.647097  [ 3216/ 7627]
Avg loss: 13.725407  [ 3536/ 7627]
Avg loss: 14.325184  [ 3856/ 7627]
Avg loss: 16.391590  [ 4176/ 7627]
Avg loss: 13.651175  [ 4496/ 7627]
Avg loss: 15.080815  [ 4816/ 7627]
Avg loss: 12.949407  [ 5136/ 7627]
Avg loss: 31.029230  [ 5456/ 7627]
Avg loss: 25.485130  [ 5776/ 7627]
Avg loss: 17.014326  [ 6096/ 7627]
Avg loss: 17.161022  [ 6416/ 7627]
Avg loss: 20.365051  [ 6736/ 7627]
Avg loss: 28.294842  [ 7056/ 7627]
Avg loss: 26.564127  [ 7376/ 7627]
Adjusting learning rate of group 0 to 8.8360e-05.
Test Error: Avg loss: 23.094005 

Epoch 3
-------------------------------2021-05-26 20:17:22.325951
Avg loss: 19.805588  [   16/ 7627]
Avg loss: 17.632587  [  336/ 7627]
Avg loss: 25.589624  [  656/ 7627]
Avg loss: 16.857376  [  976/ 7627]
Avg loss: 19.352839  [ 1296/ 7627]
Avg loss: 23.380688  [ 1616/ 7627]
Avg loss: 18.442841  [ 1936/ 7627]
Avg loss: 15.704901  [ 2256/ 7627]
Avg loss: 14.017666  [ 2576/ 7627]
Avg loss: 9.728800  [ 2896/ 7627]
Avg loss: 11.098232  [ 3216/ 7627]
Avg loss: 10.357498  [ 3536/ 7627]
Avg loss: 15.536549  [ 3856/ 7627]
Avg loss: 9.247918  [ 4176/ 7627]
Avg loss: 14.495194  [ 4496/ 7627]
Avg loss: 10.535902  [ 4816/ 7627]
Avg loss: 9.837117  [ 5136/ 7627]
Avg loss: 23.937305  [ 5456/ 7627]
Avg loss: 9.658115  [ 5776/ 7627]
Avg loss: 14.781637  [ 6096/ 7627]
Avg loss: 12.247084  [ 6416/ 7627]
Avg loss: 13.266824  [ 6736/ 7627]
Avg loss: 14.193076  [ 7056/ 7627]
Avg loss: 16.489616  [ 7376/ 7627]
Adjusting learning rate of group 0 to 8.3058e-05.
Test Error: Avg loss: 23.287162 

Epoch 4
-------------------------------2021-05-26 20:20:29.401417
Avg loss: 6.427515  [   16/ 7627]
Avg loss: 9.838905  [  336/ 7627]
Avg loss: 8.189766  [  656/ 7627]
Avg loss: 15.555811  [  976/ 7627]
Avg loss: 17.604506  [ 1296/ 7627]
Avg loss: 12.319357  [ 1616/ 7627]
Avg loss: 10.800063  [ 1936/ 7627]
Avg loss: 9.594942  [ 2256/ 7627]
Avg loss: 9.014856  [ 2576/ 7627]
Avg loss: 12.144673  [ 2896/ 7627]
Avg loss: 10.164950  [ 3216/ 7627]
Avg loss: 6.684187  [ 3536/ 7627]
Avg loss: 21.007435  [ 3856/ 7627]
Avg loss: 17.859688  [ 4176/ 7627]
Avg loss: 14.420454  [ 4496/ 7627]
Avg loss: 6.717405  [ 4816/ 7627]
Avg loss: 11.247580  [ 5136/ 7627]
Avg loss: 11.619949  [ 5456/ 7627]
Avg loss: 9.566970  [ 5776/ 7627]
Avg loss: 11.910748  [ 6096/ 7627]
Avg loss: 11.193823  [ 6416/ 7627]
Avg loss: 13.278071  [ 6736/ 7627]
Avg loss: 19.361610  [ 7056/ 7627]
Avg loss: 20.199989  [ 7376/ 7627]
Adjusting learning rate of group 0 to 7.8075e-05.
Test Error: Avg loss: 24.061855 

Epoch 5
-------------------------------2021-05-26 20:23:36.726667
Avg loss: 12.499569  [   16/ 7627]
Avg loss: 9.090734  [  336/ 7627]
Avg loss: 8.248756  [  656/ 7627]
Avg loss: 5.439269  [  976/ 7627]
Avg loss: 15.882401  [ 1296/ 7627]
Avg loss: 6.984636  [ 1616/ 7627]
Avg loss: 10.019980  [ 1936/ 7627]
Avg loss: 12.999468  [ 2256/ 7627]
Avg loss: 6.676401  [ 2576/ 7627]
Avg loss: 6.521830  [ 2896/ 7627]
Avg loss: 12.859800  [ 3216/ 7627]
Avg loss: 8.528404  [ 3536/ 7627]
Avg loss: 9.246223  [ 3856/ 7627]
Avg loss: 5.553308  [ 4176/ 7627]
Avg loss: 7.124526  [ 4496/ 7627]
Avg loss: 8.801616  [ 4816/ 7627]
Avg loss: 7.006494  [ 5136/ 7627]
Avg loss: 4.549972  [ 5456/ 7627]
Avg loss: 8.902361  [ 5776/ 7627]
Avg loss: 9.890976  [ 6096/ 7627]
Avg loss: 9.797801  [ 6416/ 7627]
Avg loss: 6.864541  [ 6736/ 7627]
Avg loss: 5.256314  [ 7056/ 7627]
Avg loss: 10.461657  [ 7376/ 7627]
Adjusting learning rate of group 0 to 7.3390e-05.
Test Error: Avg loss: 27.435639 

Epoch 6
-------------------------------2021-05-26 20:26:43.975122
Avg loss: 5.943437  [   16/ 7627]
Avg loss: 4.986811  [  336/ 7627]
Avg loss: 10.421003  [  656/ 7627]
Avg loss: 8.329754  [  976/ 7627]
Avg loss: 4.957479  [ 1296/ 7627]
Avg loss: 6.189426  [ 1616/ 7627]
Avg loss: 6.621826  [ 1936/ 7627]
Avg loss: 5.339303  [ 2256/ 7627]
Avg loss: 4.002953  [ 2576/ 7627]
Avg loss: 7.381094  [ 2896/ 7627]
Avg loss: 9.947974  [ 3216/ 7627]
Avg loss: 9.359675  [ 3536/ 7627]
Avg loss: 4.159526  [ 3856/ 7627]
Avg loss: 5.629273  [ 4176/ 7627]
Avg loss: 4.891400  [ 4496/ 7627]
Avg loss: 6.732113  [ 4816/ 7627]
Avg loss: 9.201433  [ 5136/ 7627]
Avg loss: 5.021096  [ 5456/ 7627]
Avg loss: 7.577225  [ 5776/ 7627]
Avg loss: 3.979990  [ 6096/ 7627]
Avg loss: 5.976898  [ 6416/ 7627]
Avg loss: 9.535212  [ 6736/ 7627]
Avg loss: 4.612142  [ 7056/ 7627]
Avg loss: 7.273851  [ 7376/ 7627]
Adjusting learning rate of group 0 to 6.8987e-05.
Test Error: Avg loss: 26.880053 

Epoch 7
-------------------------------2021-05-26 20:29:50.893472
Avg loss: 5.483120  [   16/ 7627]
Avg loss: 3.355362  [  336/ 7627]
Avg loss: 4.091997  [  656/ 7627]
Avg loss: 4.020205  [  976/ 7627]
Avg loss: 4.060575  [ 1296/ 7627]
Avg loss: 3.140441  [ 1616/ 7627]
Avg loss: 3.837975  [ 1936/ 7627]
Avg loss: 4.060224  [ 2256/ 7627]
Avg loss: 4.879004  [ 2576/ 7627]
Avg loss: 3.775086  [ 2896/ 7627]
Avg loss: 2.458167  [ 3216/ 7627]
Avg loss: 3.559824  [ 3536/ 7627]
Avg loss: 3.978436  [ 3856/ 7627]
Avg loss: 6.621753  [ 4176/ 7627]
Avg loss: 1.371094  [ 4496/ 7627]
Avg loss: 2.574625  [ 4816/ 7627]
Avg loss: 2.925139  [ 5136/ 7627]
Avg loss: 4.671077  [ 5456/ 7627]
Avg loss: 4.090400  [ 5776/ 7627]
Avg loss: 4.031273  [ 6096/ 7627]
Avg loss: 3.928929  [ 6416/ 7627]
Avg loss: 2.517519  [ 6736/ 7627]
Avg loss: 3.280835  [ 7056/ 7627]
Avg loss: 2.454046  [ 7376/ 7627]
Adjusting learning rate of group 0 to 6.4848e-05.
Test Error: Avg loss: 28.847137 

Epoch 8
-------------------------------2021-05-26 20:32:58.273518
Avg loss: 2.045284  [   16/ 7627]
Avg loss: 2.185203  [  336/ 7627]
Avg loss: 2.239439  [  656/ 7627]
Avg loss: 0.770612  [  976/ 7627]
Avg loss: 4.030661  [ 1296/ 7627]
