{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sonic-machine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import random\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse, os\n",
    "\n",
    "import data_generator\n",
    "import hparams\n",
    "import model_unet\n",
    "import numpy as np\n",
    "from loss_function import CrossEntropyLoss_Origin\n",
    "import utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-verse",
   "metadata": {},
   "source": [
    "# argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "copyrighted-verse",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------ArgParse--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--save_dir SAVE_DIR]\n",
      "                             [--saved_model SAVED_MODEL]\n",
      "                             [--lr LR]\n",
      "                             [-e EPOCHS]\n",
      "                             [-o OUT_FLOOR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/wangxiaoyu/.local/share/jupyter/runtime/kernel-65eee6ec-9019-493d-b0a1-cb3afc8d231b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangxiaoyu/anaconda3/envs/pytorch/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3435: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "save_dir = None\n",
    "lr = 0\n",
    "saved_model_path = None\n",
    "epochs = 0\n",
    "epochs_finished = 0\n",
    "num_floor = -1\n",
    "\n",
    "\n",
    "print('--------------ArgParse--------------')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--save_dir', help='目标存储目录')\n",
    "parser.add_argument('--saved_model', help='以训练过的模型')\n",
    "parser.add_argument('--lr', type=float, help='学习率')\n",
    "parser.add_argument('-e', '--epochs', type=int, help='有几个epoch')\n",
    "# parser.add_argument('--epochs_finished', type=int, help='已经完成了几个epoch')\n",
    "parser.add_argument('-o', '--out_floor', type=int, help='输出在第几层(0，1，2，3)')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "assert args.save_dir != None, ('请输入存储目录')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    print(f'Save_dir {args.save_dir} does not exist. Will create one.')\n",
    "    save_dir = args.save_dir\n",
    "else:\n",
    "    print(f'Save_dir {args.save_dir} already exist. Will create {args.save_dir}_2 to avoid overwrite.')\n",
    "    save_dir = args.save_dir+'_2'\n",
    "os.mkdir(save_dir)\n",
    "\n",
    "if args.saved_model == None:\n",
    "    print('Don\\'t use saved model, train from scratch')\n",
    "else:\n",
    "    print(f'Use saved model[{arg.saved_model}], train from it')\n",
    "saved_model_path = args.saved_model\n",
    "\n",
    "if args.lr == None:\n",
    "    lr = 1e-3\n",
    "    print('Using default lr=1e-3')\n",
    "else:\n",
    "    lr = args.lr\n",
    "    print(f'Using lr from command: lr={lr}')\n",
    "\n",
    "assert args.epochs != None, ('请输入epochs数')\n",
    "epochs = args.epochs\n",
    "\n",
    "assert args.out_floor != None, ('请输入输出的层数')\n",
    "assert args.out_floor in [0,1,2,3], ('输入的层数必须为0 1 2 3之一')\n",
    "num_floor = args.out_floor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-configuration",
   "metadata": {},
   "source": [
    "# split data, generate train/test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "crazy-jenny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-09 11:41:18.971062 - Preparing train_dataloader...\n",
      "2021-07-09 11:41:28.249703 - Preparing valid_dataloader...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "fold_index = list(range(10))\n",
    "random.shuffle(fold_index)\n",
    "test_fold_index = fold_index[0]\n",
    "validation_fold_index = fold_index[1]\n",
    "train_fold_index_list = fold_index[2:]\n",
    "'''\n",
    "train_fold_index_list = hparams.train_set_fold_index\n",
    "valid_fold_index_list = hparams.validation_set_fold_index\n",
    "\n",
    "# prepare dataloader\n",
    "print(f'{datetime.datetime.now()} - Preparing train_dataloader...')\n",
    "train_dataloader = data_generator.source_index_to_chunk_list(source_list=train_fold_index_list, \n",
    "                                                             data_chunks_duration_in_bins=hparams.data_chunks_duration_in_bins,\n",
    "                                                             data_chunks_overlap_in_bins=hparams.data_chunks_overlap_in_bins_for_training)#[0:32]\n",
    "print(f'{datetime.datetime.now()} - Preparing valid_dataloader...')\n",
    "valid_dataloader = data_generator.source_index_to_chunk_list(source_list=valid_fold_index_list,\n",
    "                                                             data_chunks_duration_in_bins=hparams.data_chunks_duration_in_bins,\n",
    "                                                             data_chunks_overlap_in_bins=hparams.data_chunks_overlap_in_bins_for_training)#[0:32]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataloader, batch_size=hparams.batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataloader, batch_size=hparams.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "critical-coverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24705"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "utility-building",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-marathon",
   "metadata": {},
   "source": [
    "# train/test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-karma",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, scheduler, out_floor):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    loss_total = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader): # 每次返回一个batch\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        batch_size = len(X)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X, out_floor)\n",
    "        \n",
    "        if out_floor == 0:\n",
    "            loss = loss_fn(pred, y)\n",
    "        else:\n",
    "            # downsample y\n",
    "            y_downsample = utils.downsample(y, out_floor)\n",
    "            loss = loss_fn(pred, y_downsample)\n",
    "            \n",
    "        loss_total += loss.item()\n",
    "            \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Avg loss: {loss/batch_size:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    scheduler.step()\n",
    "    return loss_total/size\n",
    "            \n",
    "def test(dataloader, model, out_floor):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X, out_floor)\n",
    "            \n",
    "            if out_floor == 0:\n",
    "                loss = loss_fn(pred, y)\n",
    "            else:\n",
    "                # downsample y\n",
    "                y_downsample = utils.downsample(y, out_floor)\n",
    "                loss = loss_fn(pred, y_downsample)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item() # TODO\n",
    "    test_loss /= size\n",
    "    #correct /= size\n",
    "    #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    print(f\"Test Error: Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-default",
   "metadata": {},
   "source": [
    "# Train :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-turning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model_unet.UNet(device=device)\n",
    "if saved_model_path != None:\n",
    "    print(f'loading model from {saved_model_path}...')\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "else:\n",
    "    print('raw model')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "scheduler_decay = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.94, verbose=True)\n",
    "scheduler_stop = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', threshold=1e-3, factor=-1, patience=1000)\n",
    "loss_fn = CrossEntropyLoss_Origin().to(device)\n",
    "\n",
    "test_loss_list = []\n",
    "train_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-eclipse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in range(epochs_finished, epochs_finished+epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------{datetime.datetime.now()}\")\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer, scheduler_decay, num_floor)\n",
    "    test_loss = test(valid_dataloader, model, num_floor)\n",
    "    \n",
    "    test_loss_list.append(test_loss)\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    scheduler_stop.step(test_loss)\n",
    "    \n",
    "    if optimizer.state_dict()['param_groups'][0]['lr']<=0:\n",
    "        print(f'Early stop after {t+1} epochs.')\n",
    "        break\n",
    "    \n",
    "    plt.plot(range(1,len(train_loss_list)+1), train_loss_list, c='b')\n",
    "    plt.plot(range(1,len(test_loss_list)+1), test_loss_list, c='r')\n",
    "    plt.savefig(os.path.join(save_dir, 'loss.png'))\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'model_lr{lr}_floor{num_floor}_epoch{t+1}.pth'))\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-things",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), 'model_minibatch_floor3_good.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    !jupyter nbconvert --to python train_the_model.ipynb\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-airline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./train_result_floor_3/0526_f3_2/model_lr0.0001_floor3_epoch2.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_floor = 3\n",
    "for idx, (X, y) in train_dataloader:\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    pred = model(X, out_floor)\n",
    "\n",
    "    if out_floor == 0:\n",
    "        loss = loss_fn(pred, y)\n",
    "    else:\n",
    "        # downsample y\n",
    "        y_downsample = utils.downsample(y, out_floor)\n",
    "        loss = loss_fn(pred, y_downsample)\n",
    "\n",
    "    test_loss += loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
