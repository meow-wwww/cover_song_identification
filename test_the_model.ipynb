{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sonic-machine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "from imp import reload\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "from  tqdm import tqdm\n",
    "import math, os\n",
    "\n",
    "import data_generator\n",
    "import hparams\n",
    "reload(hparams)\n",
    "import model_unet\n",
    "reload(model_unet)\n",
    "import numpy as np\n",
    "import loss_function\n",
    "reload(loss_function)\n",
    "import utils\n",
    "reload(utils)\n",
    "import evaluate\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occasional-karma",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 一个epoch的训练+测试\n",
    "def train(dataloader, model, loss_fn, optimizer, scheduler, out_floor):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    batch_num = math.ceil(size/dataloader.batch_size)\n",
    "    loss_total = 0\n",
    "    \n",
    "    for batch, (X, y) in tqdm(enumerate(dataloader)): # 每次返回一个batch\n",
    "        # X, y = X.cuda(device=device_ids[0]), y.cuda(device=device_ids[0])\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        # Compute prediction error\n",
    "        pred = model(X, out_floor)\n",
    "        \n",
    "        if out_floor == 0:\n",
    "            loss = loss_fn(pred, y)\n",
    "        else:\n",
    "            # downsample y\n",
    "            y_downsample = utils.downsample(y, out_floor)\n",
    "            # print(pred.shape, y_downsample.shape)\n",
    "            loss = loss_fn(pred, y_downsample)\n",
    "            \n",
    "        # 对于某些特殊的损失函数：\n",
    "        if args.loss in [2,3]:\n",
    "            loss = (loss[0].sum()+loss[2].sum())/(loss[1].sum()+loss[3].sum())\n",
    "        elif args.loss in [0,1]:\n",
    "            loss = loss.sum()\n",
    "            \n",
    "        loss_total += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch+1) % 50 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Avg loss: {loss:.4f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    scheduler.step()\n",
    "    return loss_total/batch_num\n",
    "\n",
    "threshold = 0.05\n",
    "args_loss = 3\n",
    "def test(dataloader, model, loss_fn, out_floor):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        size = len(dataloader.dataset)\n",
    "        batch_num = math.ceil(size/dataloader.batch_size)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_loss_v, test_loss_nv = 0, 0\n",
    "        test_loss_f, test_loss_t = 0, 0\n",
    "        oa_avg, vr_avg, vfa_avg, rpa_avg, rca_avg = 0, 0, 0, 0, 0\n",
    "    \n",
    "    \n",
    "        for X, y in tqdm(dataloader):\n",
    "            # X, y = X.cuda(device=device_ids[0]), y.cuda(device=device_ids[0])\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            Xpred = model(X, out_floor)\n",
    "            Xout = utils.salience_to_output(Xpred.clone().detach(), threshold=threshold)\n",
    "            \n",
    "            if out_floor == 0:\n",
    "                loss = loss_fn(Xpred, y)\n",
    "                oa, vr, vfa, rpa, rca = evaluate.evaluate(Xout, y, out_floor)\n",
    "            else:\n",
    "                y_downsample = utils.downsample(y, out_floor)\n",
    "                loss = loss_fn(Xpred, y_downsample)\n",
    "                oa, vr, vfa, rpa, rca = evaluate.evaluate(Xout, y_downsample, out_floor)\n",
    "                \n",
    "            if args_loss == 2:\n",
    "                loss_gather = (loss[0].sum()+loss[2].sum())/(loss[1].sum()+loss[3].sum())\n",
    "                loss_v_gather = loss[0].sum()/loss[1].sum()\n",
    "                loss_nv_gather = loss[2].sum()/loss[3].sum()\n",
    "                test_loss_v += loss_v_gather.item()\n",
    "                test_loss_nv += loss_nv_gather.item()\n",
    "            elif args_loss == 3:\n",
    "                loss_gather = (loss[0].sum()+loss[2].sum())/(loss[1].sum()+loss[3].sum())\n",
    "                loss_f_gather = loss[0].sum()/loss[1].sum()\n",
    "                loss_t_gather = loss[2].sum()/loss[3].sum()\n",
    "                test_loss_f += loss_f_gather.item()\n",
    "                test_loss_t += loss_t_gather.item()\n",
    "            elif args_loss in [0,1]:\n",
    "                loss_gather = loss.sum()\n",
    "            \n",
    "            if loss_gather.item() != loss_gather.item():\n",
    "                print('nan!')\n",
    "            test_loss += loss_gather.item()\n",
    "\n",
    "            \n",
    "            oa_avg += oa\n",
    "            vr_avg += vr\n",
    "            vfa_avg += vfa\n",
    "            rpa_avg += rpa\n",
    "            rca_avg += rca\n",
    "            \n",
    "    test_loss /= batch_num # 每张图的loss\n",
    "    test_loss_v /= batch_num\n",
    "    test_loss_nv /= batch_num\n",
    "    test_loss_f /= batch_num\n",
    "    test_loss_t /= batch_num\n",
    "    \n",
    "    oa_avg /= batch_num\n",
    "    vr_avg /= batch_num\n",
    "    vfa_avg /= batch_num\n",
    "    rpa_avg /= batch_num\n",
    "    rca_avg /= batch_num\n",
    "    \n",
    "    print(f\"Test Error: Avg loss: {test_loss:.4f} \\n\")\n",
    "    print(f\"Test OA\\t{oa_avg:.4f}\\tVR\\t{vr_avg:.4f}\\tVFA\\t{vfa_avg:.4f}\\tRPA\\t{rpa_avg:.4f}\\tRCA\\t{rca_avg:.4f}\\n\")\n",
    "    \n",
    "    if args_loss in [0,1,2]:\n",
    "        return (test_loss, test_loss_v, test_loss_nv), oa_avg, vr_avg, vfa_avg, rpa_avg, rca_avg\n",
    "    elif args_loss in [3]:\n",
    "        return (test_loss, test_loss_f, test_loss_t), oa_avg, vr_avg, vfa_avg, rpa_avg, rca_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-configuration",
   "metadata": {},
   "source": [
    "# split data, generate train/test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "failing-stable",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 划分数据\n",
    "'''\n",
    "fold_index = list(range(10))\n",
    "random.shuffle(fold_index)\n",
    "test_fold_index = fold_index[0]\n",
    "validation_fold_index = fold_index[1]\n",
    "train_fold_index_list = fold_index[2:]\n",
    "'''\n",
    "train_fold_index_list = hparams.train_set_fold_index\n",
    "valid_fold_index_list = hparams.validation_set_fold_index\n",
    "test_fold_index_list = hparams.test_set_fold_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "crazy-jenny",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-12 09:15:23.976486 - Preparing valid_dataloader...\n",
      "2021-08-12 09:15:24.768596 - Preparing test_dataloader...\n"
     ]
    }
   ],
   "source": [
    "# prepare\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "'''print(f'{datetime.datetime.now()} - Preparing train_dataloader...')\n",
    "train_dataloader = data_generator.source_index_to_chunk_list(source_list=train_fold_index_list,\n",
    "                                                             data_chunks_duration_in_bins=hparams.data_chunks_duration_in_bins,\n",
    "                                                             data_chunks_overlap_in_bins=hparams.data_chunks_overlap_in_bins_for_training)\n",
    "train_dataloader = DataLoader(train_dataloader, batch_size=BATCH_SIZE, shuffle=True)'''\n",
    "\n",
    "\n",
    "print(f'{datetime.datetime.now()} - Preparing valid_dataloader...')\n",
    "valid_dataloader = data_generator.source_index_to_chunk_list(source_list=valid_fold_index_list,\n",
    "                                                             data_chunks_duration_in_bins=hparams.data_chunks_duration_in_bins,\n",
    "                                                             data_chunks_overlap_in_bins=hparams.data_chunks_overlap_in_bins_for_training)#[0:32]\n",
    "valid_dataloader = DataLoader(valid_dataloader, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f'{datetime.datetime.now()} - Preparing test_dataloader...')\n",
    "test_dataloader = data_generator.source_index_to_chunk_list(source_list=test_fold_index_list,\n",
    "                                                             data_chunks_duration_in_bins=hparams.data_chunks_duration_in_bins,\n",
    "                                                             data_chunks_overlap_in_bins=hparams.data_chunks_overlap_in_bins_for_training)#[0:32]\n",
    "test_dataloader = DataLoader(test_dataloader, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "involved-average",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:00<?, ?it/s]/home/yangds/anaconda3/envs/csi/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|██████████| 49/49 [00:16<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: Avg loss: 7.3482 \n",
      "\n",
      "Test OA\t0.5356\tVR\t0.8038\tVFA\t0.3786\tRPA\t0.5088\tRCA\t0.6357\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7.348228182111468, 11.75036519887496, 1.772625850171459),\n",
       " 0.5356351843062805,\n",
       " 0.8038004057293274,\n",
       " 0.3785576404272682,\n",
       " 0.5088360445980709,\n",
       " 0.6356629938239958)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试模型在测试集上的效果\n",
    "model = model_unet.UNet().cuda()\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "model.load_state_dict(torch.load('./model/floor1R/model_floor1_best.pth'))\n",
    "\n",
    "# test set result?\n",
    "loss_fn = loss_function.CrossEntropyLoss_for_FA_CE_TF().cuda()\n",
    "loss_fn = nn.DataParallel(loss_fn)\n",
    "\n",
    "test(test_dataloader, model, loss_fn, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-compound",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 测试模型在测试集上的效果\n",
    "model = model_unet.UNet().cuda()\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "model.load_state_dict(torch.load('./model/F3/model_0.pth'))\n",
    "\n",
    "# model = torch.load('./model/test/model_floor3_best.pth')\n",
    "\n",
    "# test set result?\n",
    "loss_fn = loss_function.CrossEntropyLoss_for_FA_CE_TF().cuda()\n",
    "loss_fn = nn.DataParallel(loss_fn)\n",
    "\n",
    "test(test_dataloader, model, loss_fn, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-wonder",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# 逐个比较损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "korean-creator",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "reload(evaluate)\n",
    "\n",
    "out_floor = 2\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-pottery",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show\n",
    "nanX, nany = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for a, b in valid_dataloader:\n",
    "        X, y = a.cuda(), b.cuda()\n",
    "\n",
    "        X = X[0:2]\n",
    "        y = y[0:2]\n",
    "        \n",
    "        Xpred = model(X, out_floor)\n",
    "        Xshow = utils.salience_to_output(Xpred.clone().detach(), threshold=0.05)\n",
    "        y_downsample = utils.downsample(y, out_floor).int()\n",
    "        # print(f'Xpred: {Xpred.shape}')\n",
    "        # print(f'y_downsample: {y_downsample.shape}')\n",
    "        loss = loss_fn(Xpred, y_downsample)\n",
    "        # print(loss)\n",
    "        loss = (loss[0].sum()+loss[2].sum())/(loss[1].sum()+loss[3].sum())\n",
    "        print(loss)\n",
    "        print(evaluate.evaluate(Xshow, y_downsample, out_floor))\n",
    "        \n",
    "        plt.figure(figsize=(10,14))\n",
    "        plt.subplot(241)\n",
    "        plt.imshow(X[0][0].cpu(), origin='lower')\n",
    "        plt.subplot(242)\n",
    "        plt.imshow(Xpred[0][0].cpu(), origin='lower')\n",
    "        plt.subplot(243)\n",
    "        plt.imshow(Xshow[0][0].cpu(), origin='lower')\n",
    "        plt.subplot(244)\n",
    "        plt.imshow(y_downsample[0].cpu(), origin='lower')\n",
    "        \n",
    "        plt.subplot(245)\n",
    "        plt.imshow(X[1][0].cpu(), origin='lower')\n",
    "        plt.subplot(246)\n",
    "        plt.imshow(Xpred[1][0].cpu(), origin='lower')\n",
    "        plt.subplot(247)\n",
    "        plt.imshow(Xshow[1][0].cpu(), origin='lower')\n",
    "        plt.subplot(248)\n",
    "        plt.imshow(y_downsample[1].cpu(), origin='lower')\n",
    "        plt.show()\n",
    "        \n",
    "        print('------------')\n",
    "        \n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-prime",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X, y = torch.load('Xy.pt')\n",
    "\n",
    "    gpu = 2\n",
    "    X = X[16*gpu:16*(gpu+1)]\n",
    "    y = y[16*gpu:16*(gpu+1)]\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    Xpred = model(X, out_floor)\n",
    "    y_downsample = utils.downsample(y, out_floor).int()\n",
    "    loss = loss_fn(Xpred, y_downsample)\n",
    "    print(loss)\n",
    "    loss = (loss[0].sum()+loss[2].sum())/(loss[1].sum()+loss[3].sum())\n",
    "    print(loss)\n",
    "\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-grenada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-distinction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-sample",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(131)\n",
    "plt.imshow(Xpred[0][0].cpu(), origin='lower')\n",
    "plt.subplot(132)\n",
    "plt.imshow(Xshow[0][0].cpu(), origin='lower')\n",
    "plt.subplot(133)\n",
    "plt.imshow(y_downsample[0].cpu(), origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(Xpred, y_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_function.CrossEntropyLoss_Origin as loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-database",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for X, y in train_dataloader:\n",
    "\n",
    "    y_downsample = utils.downsample(y, 3)\n",
    "    \n",
    "    break\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.imshow(y[0], origin='lower')\n",
    "plt.subplot(232)\n",
    "plt.imshow(y_downsample[0], origin='lower')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.imshow(y[1], origin='lower')\n",
    "plt.subplot(235)\n",
    "plt.imshow(y_downsample[1], origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-fountain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-starter",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss_Origin().to(device)\n",
    "\n",
    "model.eval()\n",
    "model2.eval()\n",
    "\n",
    "X, y = None, None\n",
    "for a,b in tqdm(train_dataloader):\n",
    "    X, y = a.to(device), b.to(device)\n",
    "\n",
    "    Xpred = model(X, 2)\n",
    "    y_downsample = utils.downsample(y, 2)\n",
    "\n",
    "    temp = Xpred[0,0].detach().to('cpu').numpy()\n",
    "\n",
    "    clean = utils.salience_to_output(temp, threshold=0)\n",
    "\n",
    "\n",
    "    Xpred2 = model2(X, 2)\n",
    "\n",
    "    temp2 = Xpred2[0,0].detach().to('cpu').numpy()\n",
    "\n",
    "    clean2 = utils.salience_to_output(temp2, threshold=0)\n",
    "    \n",
    "    if loss_fn(Xpred, y_downsample) + 10 < loss_fn(Xpred2, y_downsample):\n",
    "    \n",
    "        plt.subplot(231)\n",
    "        plt.imshow(y_downsample[0].detach().to('cpu').numpy(), origin='lower')\n",
    "        plt.subplot(232)\n",
    "        plt.imshow(temp, origin='lower')\n",
    "        plt.subplot(233)\n",
    "        plt.imshow(clean, origin='lower')\n",
    "\n",
    "        plt.subplot(234)\n",
    "        plt.imshow(y_downsample[0].detach().to('cpu').numpy(), origin='lower')\n",
    "        plt.subplot(235)\n",
    "        plt.imshow(temp2, origin='lower')\n",
    "        plt.subplot(236)\n",
    "        plt.imshow(clean2, origin='lower')\n",
    "        plt.show()\n",
    "        print(loss_fn(Xpred, y_downsample) , loss_fn(Xpred2, y_downsample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-brief",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_12epoch.pth')\n",
    "\n",
    "model = model_unet.UNet(device=device)\n",
    "model.load_state_dict(torch.load('model_9epoch.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-airline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    !jupyter nbconvert --to python train_the_model.ipynb\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
