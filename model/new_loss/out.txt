Using cuda device
--------------ArgParse--------------
Save_dir model/new_loss already exist. Will save to this directory.
Don't use saved model, train from scratch
Using lr from command: lr=1e-06
2021-07-13 14:28:03.486744 - Preparing train_dataloader...
2021-07-13 14:28:11.906232 - Preparing valid_dataloader...
raw model
Adjusting learning rate of group 0 to 1.0000e-06.
Epoch 1
-------------------------------2021-07-13 14:28:14.351142
Avg loss: 1782.6438  [ 1600/24705]
Avg loss: 1687.7667  [ 3200/24705]
Avg loss: 1433.0725  [ 4800/24705]
Avg loss: 1415.9280  [ 6400/24705]
Avg loss: 1311.7941  [ 8000/24705]
Avg loss: 1529.5708  [ 9600/24705]
Avg loss: 1288.8723  [11200/24705]
Avg loss: 1228.3690  [12800/24705]
Avg loss: 1592.9009  [14400/24705]
Avg loss: 1342.2861  [16000/24705]
Avg loss: 1389.1409  [17600/24705]
Avg loss: 1201.7632  [19200/24705]
Avg loss: 1432.5168  [20800/24705]
Avg loss: 1443.9342  [22400/24705]
Avg loss: 1045.2986  [24000/24705]
Adjusting learning rate of group 0 to 9.4000e-07.
Test Error: Avg loss: 1343.1055 

Test OA	0.3743	VR	0.9973	VFA	0.9692	RPA	0.5071	RCA	0.5203

[in epoch 1, OA got new best: from 0 to 0.37431349009217846. Will save the model]
Epoch 2
-------------------------------2021-07-13 14:36:51.800113
Avg loss: 1254.3362  [ 1600/24705]
Avg loss: 1359.4810  [ 3200/24705]
Avg loss: 1430.8931  [ 4800/24705]
Avg loss: 1165.0767  [ 6400/24705]
Avg loss: 1317.4716  [ 8000/24705]
Avg loss: 1379.1584  [ 9600/24705]
Avg loss: 1261.2017  [11200/24705]
Avg loss: 1161.1951  [12800/24705]
Avg loss: 1279.5591  [14400/24705]
Avg loss: 1132.3936  [16000/24705]
Avg loss: 1353.8718  [17600/24705]
Avg loss: 1326.9739  [19200/24705]
Avg loss: 1303.0194  [20800/24705]
Avg loss: 1407.2407  [22400/24705]
Avg loss: 1473.7568  [24000/24705]
Adjusting learning rate of group 0 to 8.8360e-07.
Test Error: Avg loss: 1304.5281 

Test OA	0.3876	VR	0.9990	VFA	0.9778	RPA	0.5300	RCA	0.5443

[in epoch 2, OA got new best: from 0.37431349009217846 to 0.3876331622233261. Will save the model]
Epoch 3
-------------------------------2021-07-13 14:45:30.531092
Avg loss: 1173.3173  [ 1600/24705]
Avg loss: 1173.9186  [ 3200/24705]
Avg loss: 1454.8257  [ 4800/24705]
Avg loss: 1209.9998  [ 6400/24705]
Avg loss: 1176.4078  [ 8000/24705]
Avg loss: 1108.7517  [ 9600/24705]
Avg loss: 1183.8851  [11200/24705]
Avg loss: 1081.4819  [12800/24705]
Avg loss: 1074.5697  [14400/24705]
Avg loss: 1152.8599  [16000/24705]
Avg loss: 1070.0277  [17600/24705]
Avg loss: 1304.5923  [19200/24705]
Avg loss: 1096.1238  [20800/24705]
Avg loss: 1302.3149  [22400/24705]
Avg loss: 1276.7156  [24000/24705]
Adjusting learning rate of group 0 to 8.3058e-07.
Test Error: Avg loss: 1286.0357 

Test OA	0.3993	VR	0.9995	VFA	0.9819	RPA	0.5477	RCA	0.5617

[in epoch 3, OA got new best: from 0.3876331622233261 to 0.3992796820665673. Will save the model]
Epoch 4
-------------------------------2021-07-13 14:54:09.053364
Avg loss: 1177.9866  [ 1600/24705]
Avg loss: 1154.8125  [ 3200/24705]
Avg loss: 1372.0522  [ 4800/24705]
Avg loss: 1076.2808  [ 6400/24705]
Avg loss: 1403.0813  [ 8000/24705]
Avg loss: 1271.1549  [ 9600/24705]
Avg loss: 1356.2478  [11200/24705]
Avg loss: 1224.1930  [12800/24705]
Avg loss: 1286.6560  [14400/24705]
Avg loss: 1287.3667  [16000/24705]
Avg loss: 1318.5668  [17600/24705]
Avg loss: 1217.2242  [19200/24705]
Avg loss: 1148.8438  [20800/24705]
Avg loss: 1174.5732  [22400/24705]
Avg loss: 1099.1985  [24000/24705]
Adjusting learning rate of group 0 to 7.8075e-07.
Test Error: Avg loss: 1269.4977 

Test OA	0.4084	VR	0.9994	VFA	0.9764	RPA	0.5586	RCA	0.5719

[in epoch 4, OA got new best: from 0.3992796820665673 to 0.4084285477728099. Will save the model]
Epoch 5
-------------------------------2021-07-13 15:02:45.601290
Avg loss: 1203.9559  [ 1600/24705]
Avg loss: 1135.7507  [ 3200/24705]
Avg loss: 1147.8855  [ 4800/24705]
Avg loss: 1169.7672  [ 6400/24705]
Avg loss: 1225.4203  [ 8000/24705]
Avg loss: 1198.0649  [ 9600/24705]
Avg loss: 980.9202  [11200/24705]
Avg loss: 1174.2751  [12800/24705]
Avg loss: 1167.7317  [14400/24705]
Avg loss: 1454.3724  [16000/24705]
Avg loss: 1325.1792  [17600/24705]
Avg loss: 1017.7000  [19200/24705]
Avg loss: 1520.8583  [20800/24705]
Avg loss: 1150.2378  [22400/24705]
Avg loss: 1270.9303  [24000/24705]
Adjusting learning rate of group 0 to 7.3390e-07.
Test Error: Avg loss: 1266.5948 

Test OA	0.4101	VR	0.9997	VFA	0.9775	RPA	0.5630	RCA	0.5764

[in epoch 5, OA got new best: from 0.4084285477728099 to 0.4100534028812718. Will save the model]
Epoch 6
-------------------------------2021-07-13 15:11:18.067708
Avg loss: 1527.2708  [ 1600/24705]
Avg loss: 1117.4929  [ 3200/24705]
Avg loss: 1460.4121  [ 4800/24705]
Avg loss: 1022.2834  [ 6400/24705]
Avg loss: 1214.1624  [ 8000/24705]
Avg loss: 1511.0312  [ 9600/24705]
Avg loss: 943.9673  [11200/24705]
Avg loss: 1276.3452  [12800/24705]
Avg loss: 1283.1678  [14400/24705]
Avg loss: 1312.5890  [16000/24705]
Avg loss: 1212.8726  [17600/24705]
Avg loss: 1056.6885  [19200/24705]
Avg loss: 1054.5549  [20800/24705]
Avg loss: 1279.4370  [22400/24705]
Avg loss: 1286.9663  [24000/24705]
Adjusting learning rate of group 0 to 6.8987e-07.
Test Error: Avg loss: 1257.9479 

Test OA	0.4160	VR	0.9998	VFA	0.9764	RPA	0.5703	RCA	0.5839

[in epoch 6, OA got new best: from 0.4100534028812718 to 0.416014654744163. Will save the model]
Epoch 7
-------------------------------2021-07-13 15:19:50.005634
Avg loss: 1171.1147  [ 1600/24705]
Avg loss: 1447.3201  [ 3200/24705]
Avg loss: 1221.5327  [ 4800/24705]
Avg loss: 1106.2426  [ 6400/24705]
Avg loss: 972.3864  [ 8000/24705]
Avg loss: 1163.1187  [ 9600/24705]
Avg loss: 872.5925  [11200/24705]
Avg loss: 1265.8635  [12800/24705]
Avg loss: 1010.6498  [14400/24705]
Avg loss: 1133.3242  [16000/24705]
Avg loss: 1192.9462  [17600/24705]
Avg loss: 1319.5631  [19200/24705]
Avg loss: 1153.9026  [20800/24705]
Avg loss: 1231.9095  [22400/24705]
Avg loss: 1072.7151  [24000/24705]
Adjusting learning rate of group 0 to 6.4848e-07.
Test Error: Avg loss: 1247.7427 

Test OA	0.4231	VR	0.9994	VFA	0.9671	RPA	0.5769	RCA	0.5896

[in epoch 7, OA got new best: from 0.416014654744163 to 0.42306259314456035. Will save the model]
Epoch 8
-------------------------------2021-07-13 15:28:22.700227
Avg loss: 1136.2637  [ 1600/24705]
Avg loss: 1437.2397  [ 3200/24705]
Avg loss: 1426.2913  [ 4800/24705]
Avg loss: 1098.9369  [ 6400/24705]
Avg loss: 1076.1621  [ 8000/24705]
Avg loss: 1124.9246  [ 9600/24705]
Avg loss: 1117.2422  [11200/24705]
Avg loss: 1047.5090  [12800/24705]
Avg loss: 1005.1226  [14400/24705]
Avg loss: 933.2262  [16000/24705]
Avg loss: 885.5151  [17600/24705]
Avg loss: 1078.3228  [19200/24705]
Avg loss: 1149.6353  [20800/24705]
Avg loss: 1460.1166  [22400/24705]
Avg loss: 1226.6047  [24000/24705]
Adjusting learning rate of group 0 to 6.0957e-07.
Test Error: Avg loss: 1248.2811 

Test OA	0.4200	VR	0.9999	VFA	0.9772	RPA	0.5764	RCA	0.5899

Epoch 9
-------------------------------2021-07-13 15:36:54.779181
Avg loss: 1148.7559  [ 1600/24705]
Avg loss: 1010.7576  [ 3200/24705]
Avg loss: 1128.5421  [ 4800/24705]
Avg loss: 1162.7836  [ 6400/24705]
Avg loss: 1490.5861  [ 8000/24705]
Avg loss: 1123.5927  [ 9600/24705]
Avg loss: 1123.9877  [11200/24705]
Avg loss: 1075.3801  [12800/24705]
Avg loss: 1449.6646  [14400/24705]
Avg loss: 1384.9246  [16000/24705]
Avg loss: 1145.5968  [17600/24705]
Avg loss: 1034.5121  [19200/24705]
Avg loss: 956.9863  [20800/24705]
Avg loss: 1424.3284  [22400/24705]
Avg loss: 1004.3671  [24000/24705]
Adjusting learning rate of group 0 to 5.7299e-07.
Test Error: Avg loss: 1245.4384 

Test OA	0.4231	VR	0.9999	VFA	0.9757	RPA	0.5799	RCA	0.5933

[in epoch 9, OA got new best: from 0.42306259314456035 to 0.42310054092840976. Will save the model]
Epoch 10
-------------------------------2021-07-13 15:45:28.097351
Avg loss: 1054.9818  [ 1600/24705]
Avg loss: 928.4821  [ 3200/24705]
Avg loss: 940.3217  [ 4800/24705]
Avg loss: 969.7113  [ 6400/24705]
Avg loss: 1293.6172  [ 8000/24705]
Avg loss: 1391.5066  [ 9600/24705]
Avg loss: 1170.4260  [11200/24705]
Avg loss: 1256.0234  [12800/24705]
Avg loss: 1266.2234  [14400/24705]
Avg loss: 1079.2217  [16000/24705]
Avg loss: 1174.3101  [17600/24705]
Avg loss: 1074.6384  [19200/24705]
Avg loss: 1095.7708  [20800/24705]
Avg loss: 1161.0452  [22400/24705]
Avg loss: 1079.3496  [24000/24705]
Adjusting learning rate of group 0 to 5.3862e-07.
Test Error: Avg loss: 1237.2916 

Test OA	0.4288	VR	0.9997	VFA	0.9703	RPA	0.5862	RCA	0.5988

[in epoch 10, OA got new best: from 0.42310054092840976 to 0.4288237566926092. Will save the model]
Epoch 11
-------------------------------2021-07-13 15:54:01.259308
Avg loss: 1064.9573  [ 1600/24705]
Avg loss: 994.5710  [ 3200/24705]
Avg loss: 1077.8676  [ 4800/24705]
Avg loss: 1074.4805  [ 6400/24705]
Avg loss: 1127.1608  [ 8000/24705]
Avg loss: 1515.1465  [ 9600/24705]
Avg loss: 1019.8808  [11200/24705]
Avg loss: 1250.7224  [12800/24705]
Avg loss: 1168.5322  [14400/24705]
Avg loss: 1053.8031  [16000/24705]
Avg loss: 1330.5237  [17600/24705]
Avg loss: 1032.4224  [19200/24705]
Avg loss: 933.5703  [20800/24705]
Avg loss: 1193.3177  [22400/24705]
Avg loss: 1364.6804  [24000/24705]
Adjusting learning rate of group 0 to 5.0630e-07.
Test Error: Avg loss: 1240.0020 

Test OA	0.4267	VR	0.9998	VFA	0.9651	RPA	0.5819	RCA	0.5953

Epoch 12
-------------------------------2021-07-13 16:02:34.181600
Avg loss: 1374.1963  [ 1600/24705]
Avg loss: 880.0847  [ 3200/24705]
Avg loss: 1135.7267  [ 4800/24705]
Avg loss: 1166.1980  [ 6400/24705]
Avg loss: 1127.6530  [ 8000/24705]
Avg loss: 1228.3861  [ 9600/24705]
Avg loss: 919.4232  [11200/24705]
Avg loss: 1063.3003  [12800/24705]
Avg loss: 1093.5199  [14400/24705]
Avg loss: 908.1375  [16000/24705]
Avg loss: 1393.0183  [17600/24705]
Avg loss: 980.2330  [19200/24705]
Avg loss: 908.6041  [20800/24705]
Avg loss: 1574.1968  [22400/24705]
Avg loss: 1117.9128  [24000/24705]
Adjusting learning rate of group 0 to 4.7592e-07.
Test Error: Avg loss: 1236.4839 

Test OA	0.4294	VR	0.9998	VFA	0.9615	RPA	0.5852	RCA	0.5980

[in epoch 12, OA got new best: from 0.4288237566926092 to 0.4293860738532871. Will save the model]
Epoch 13
-------------------------------2021-07-13 16:11:06.950238
Avg loss: 1291.9034  [ 1600/24705]
Avg loss: 1030.7893  [ 3200/24705]
Avg loss: 867.5665  [ 4800/24705]
Avg loss: 1146.9434  [ 6400/24705]
Avg loss: 1203.7395  [ 8000/24705]
Avg loss: 1089.2134  [ 9600/24705]
Avg loss: 1041.3826  [11200/24705]
Avg loss: 882.3834  [12800/24705]
Avg loss: 851.2612  [14400/24705]
Avg loss: 813.5472  [16000/24705]
Avg loss: 865.4160  [17600/24705]
Avg loss: 951.5417  [19200/24705]
Avg loss: 1199.4404  [20800/24705]
Avg loss: 1214.1726  [22400/24705]
Avg loss: 1501.3273  [24000/24705]
Adjusting learning rate of group 0 to 4.4737e-07.
Test Error: Avg loss: 1234.1782 

Test OA	0.4296	VR	0.9998	VFA	0.9693	RPA	0.5875	RCA	0.6001

[in epoch 13, OA got new best: from 0.4293860738532871 to 0.4295689131754706. Will save the model]
Epoch 14
-------------------------------2021-07-13 16:19:39.423708
Avg loss: 1202.1521  [ 1600/24705]
Avg loss: 1329.4717  [ 3200/24705]
Avg loss: 952.2549  [ 4800/24705]
Avg loss: 1247.9585  [ 6400/24705]
Avg loss: 1080.2566  [ 8000/24705]
Avg loss: 1179.3853  [ 9600/24705]
Avg loss: 940.0898  [11200/24705]
Avg loss: 1169.9183  [12800/24705]
Avg loss: 1114.4966  [14400/24705]
Avg loss: 1306.1583  [16000/24705]
Avg loss: 1005.5670  [17600/24705]
Avg loss: 1243.7314  [19200/24705]
Avg loss: 919.8817  [20800/24705]
Avg loss: 972.1810  [22400/24705]
Avg loss: 1014.8538  [24000/24705]
Adjusting learning rate of group 0 to 4.2052e-07.
Test Error: Avg loss: 1233.1093 

Test OA	0.4310	VR	0.9995	VFA	0.9629	RPA	0.5873	RCA	0.6008

[in epoch 14, OA got new best: from 0.4295689131754706 to 0.43100747916321674. Will save the model]
Epoch 15
-------------------------------2021-07-13 16:28:12.487071
Avg loss: 1256.4023  [ 1600/24705]
Avg loss: 1123.7404  [ 3200/24705]
Avg loss: 957.7856  [ 4800/24705]
Avg loss: 830.7404  [ 6400/24705]
Avg loss: 1179.6069  [ 8000/24705]
Avg loss: 1435.5259  [ 9600/24705]
Avg loss: 936.3874  [11200/24705]
Avg loss: 787.8648  [12800/24705]
Avg loss: 1237.1616  [14400/24705]
Avg loss: 1196.7798  [16000/24705]
Avg loss: 1284.1450  [17600/24705]
Avg loss: 1026.3519  [19200/24705]
Avg loss: 962.3149  [20800/24705]
Avg loss: 1242.4535  [22400/24705]
Avg loss: 1350.0815  [24000/24705]
Adjusting learning rate of group 0 to 3.9529e-07.
Test Error: Avg loss: 1231.6122 

Test OA	0.4351	VR	0.9996	VFA	0.9600	RPA	0.5931	RCA	0.6052

[in epoch 15, OA got new best: from 0.43100747916321674 to 0.43513688800574046. Will save the model]
Epoch 16
-------------------------------2021-07-13 16:36:48.580705
Avg loss: 960.1680  [ 1600/24705]
Avg loss: 960.4288  [ 3200/24705]
Avg loss: 1230.9434  [ 4800/24705]
Avg loss: 1241.3092  [ 6400/24705]
Avg loss: 1346.0654  [ 8000/24705]
Avg loss: 940.4130  [ 9600/24705]
Avg loss: 1169.8766  [11200/24705]
Avg loss: 979.5840  [12800/24705]
Avg loss: 1082.6190  [14400/24705]
Avg loss: 1137.1829  [16000/24705]
Avg loss: 1198.2759  [17600/24705]
Avg loss: 1089.1649  [19200/24705]
Avg loss: 935.6214  [20800/24705]
Avg loss: 990.8794  [22400/24705]
Avg loss: 1048.6030  [24000/24705]
Adjusting learning rate of group 0 to 3.7157e-07.
Test Error: Avg loss: 1230.8881 

Test OA	0.4341	VR	0.9996	VFA	0.9564	RPA	0.5892	RCA	0.6019

Epoch 17
-------------------------------2021-07-13 16:45:21.475707
Avg loss: 1198.7291  [ 1600/24705]
Avg loss: 938.5564  [ 3200/24705]
Avg loss: 1063.8574  [ 4800/24705]
Avg loss: 1174.6733  [ 6400/24705]
Avg loss: 918.1826  [ 8000/24705]
Avg loss: 776.5238  [ 9600/24705]
Avg loss: 1055.5492  [11200/24705]
Avg loss: 995.9836  [12800/24705]
Avg loss: 1004.3101  [14400/24705]
Avg loss: 1406.5828  [16000/24705]
Avg loss: 941.5191  [17600/24705]
Avg loss: 1076.9836  [19200/24705]
Avg loss: 1384.2208  [20800/24705]
Avg loss: 1048.6571  [22400/24705]
Avg loss: 909.7355  [24000/24705]
Adjusting learning rate of group 0 to 3.4928e-07.
Test Error: Avg loss: 1230.8024 

Test OA	0.4339	VR	0.9997	VFA	0.9602	RPA	0.5915	RCA	0.6041

Epoch 18
-------------------------------2021-07-13 16:53:54.577627
Avg loss: 1194.1633  [ 1600/24705]
Avg loss: 665.7684  [ 3200/24705]
Avg loss: 990.7465  [ 4800/24705]
Avg loss: 881.2501  [ 6400/24705]
Avg loss: 1056.5323  [ 8000/24705]
Avg loss: 1270.9802  [ 9600/24705]
Avg loss: 1028.1606  [11200/24705]
Avg loss: 953.5096  [12800/24705]
Avg loss: 1003.0585  [14400/24705]
Avg loss: 887.7403  [16000/24705]
Avg loss: 1062.1461  [17600/24705]
Avg loss: 1042.9619  [19200/24705]
Avg loss: 1147.9259  [20800/24705]
Avg loss: 1061.5964  [22400/24705]
Avg loss: 1145.1755  [24000/24705]
Adjusting learning rate of group 0 to 3.2832e-07.
Test Error: Avg loss: 1235.2932 

Test OA	0.4292	VR	0.9999	VFA	0.9631	RPA	0.5854	RCA	0.5990

Epoch 19
-------------------------------2021-07-13 17:02:31.444040
Avg loss: 1130.5581  [ 1600/24705]
Avg loss: 1006.1891  [ 3200/24705]
Avg loss: 1164.4890  [ 4800/24705]
Avg loss: 932.7562  [ 6400/24705]
Avg loss: 1174.4364  [ 8000/24705]
Avg loss: 1194.0930  [ 9600/24705]
Avg loss: 1338.3270  [11200/24705]
Avg loss: 970.1711  [12800/24705]
Avg loss: 962.4147  [14400/24705]
Avg loss: 1127.3136  [16000/24705]
Avg loss: 1069.8154  [17600/24705]
Avg loss: 1285.7478  [19200/24705]
Avg loss: 1215.6069  [20800/24705]
Avg loss: 1256.5043  [22400/24705]
Avg loss: 1136.3580  [24000/24705]
Adjusting learning rate of group 0 to 3.0862e-07.
Test Error: Avg loss: 1228.6307 

Test OA	0.4360	VR	0.9997	VFA	0.9563	RPA	0.5911	RCA	0.6035

[in epoch 19, OA got new best: from 0.43513688800574046 to 0.4359613898548325. Will save the model]
Epoch 20
-------------------------------2021-07-13 17:11:09.629895
Avg loss: 1171.4635  [ 1600/24705]
Avg loss: 1048.7914  [ 3200/24705]
Avg loss: 985.0026  [ 4800/24705]
Avg loss: 1040.2637  [ 6400/24705]
Avg loss: 994.1617  [ 8000/24705]
Avg loss: 1067.6703  [ 9600/24705]
Avg loss: 1481.5605  [11200/24705]
Avg loss: 1075.9341  [12800/24705]
Avg loss: 1025.7598  [14400/24705]
Avg loss: 1183.8662  [16000/24705]
Avg loss: 1250.5176  [17600/24705]
Avg loss: 702.4915  [19200/24705]
Avg loss: 1025.4777  [20800/24705]
Avg loss: 1257.2075  [22400/24705]
Avg loss: 998.3322  [24000/24705]
Adjusting learning rate of group 0 to 2.9011e-07.
Test Error: Avg loss: 1230.6723 

Test OA	0.4350	VR	0.9996	VFA	0.9593	RPA	0.5903	RCA	0.6027

Done!
